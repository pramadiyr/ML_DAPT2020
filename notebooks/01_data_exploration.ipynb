{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrusion Detection - Data Exploration and Preprocessing\n",
    "\n",
    "This notebook will guide you through:\n",
    "- Loading and combining the DAPT 2020 dataset (10 CSV files)\n",
    "- Exploring the data\n",
    "- Preprocessing for ML\n",
    "- Building a simple binary classifier (benign vs attack)\n",
    "\n",
    "The DAPT 2020 dataset contains network flow data from different days and network segments:\n",
    "- Public and private network traffic\n",
    "- Different days (Monday-Friday)\n",
    "- Normal and attack traffic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c1a71",
   "metadata": {},
   "source": [
    "## 1. Loading and Combining the Dataset\n",
    "\n",
    "The DAPT 2020 dataset consists of multiple CSV files. Let's load and combine them all for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to CSV files\n",
    "data_path = '../data/csv/'\n",
    "\n",
    "# Get list of all CSV files\n",
    "csv_files = glob.glob(os.path.join(data_path, '*.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "print(f\"\\nFiles to combine: {len(csv_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and combine all CSV files\n",
    "def load_dapt_dataset(data_path):\n",
    "    \"\"\"\n",
    "    Load and combine all DAPT 2020 CSV files\n",
    "    Returns combined dataframe\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(data_path, '*.csv'))\n",
    "    dataframes = []\n",
    "    \n",
    "    print(\"Loading CSV files...\")\n",
    "    for i, file in enumerate(csv_files, 1):\n",
    "        filename = os.path.basename(file)\n",
    "        print(f\"  {i}/{len(csv_files)}: Loading {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            # Add source file information\n",
    "            df['source_file'] = filename\n",
    "            \n",
    "            # Extract day and network type from filename\n",
    "            if 'monday' in filename:\n",
    "                df['day'] = 'Monday'\n",
    "            elif 'tuesday' in filename:\n",
    "                df['day'] = 'Tuesday'\n",
    "            elif 'wednesday' in filename:\n",
    "                df['day'] = 'Wednesday'\n",
    "            elif 'thursday' in filename:\n",
    "                df['day'] = 'Thursday'\n",
    "            elif 'friday' in filename:\n",
    "                df['day'] = 'Friday'\n",
    "            else:\n",
    "                df['day'] = 'Unknown'\n",
    "                \n",
    "            if 'pvt' in filename:\n",
    "                df['network_type'] = 'Private'\n",
    "            elif 'public' in filename:\n",
    "                df['network_type'] = 'Public'\n",
    "            else:\n",
    "                df['network_type'] = 'Mixed'\n",
    "                \n",
    "            dataframes.append(df)\n",
    "            print(f\"    → Loaded {len(df):,} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Error loading {filename}: {e}\")\n",
    "    \n",
    "    if dataframes:\n",
    "        # Combine all dataframes\n",
    "        print(f\"\\nCombining {len(dataframes)} dataframes...\")\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        print(f\"✓ Combined dataset: {len(combined_df):,} total rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No valid CSV files found!\")\n",
    "        return None\n",
    "\n",
    "# Load the combined dataset\n",
    "df = load_dapt_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af3fb9",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration\n",
    "\n",
    "Let's examine the structure and content of our combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "\n",
    "print(\"\\n=== FIRST FEW ROWS ===\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
